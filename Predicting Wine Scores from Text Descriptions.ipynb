{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Wine Scores from Text Descriptions \n",
    "\n",
    "Accompanying medium article "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "data = pd.read_csv(\"winemag-data_first150k.csv\", index_col = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining data integrity at a high level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine points are between 80 to 100 (the scale they use is from 1-100); this is skewed to the right \n",
    "# From my limited experience with wine, prices seem shifted to the right as well\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No missing entries for points and description\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# set seaborn style \n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "def clean_description(desc):\n",
    "    desc = word_tokenize(desc.lower())\n",
    "    desc = [token for token in desc if token not in stopwords and token.isalpha()]\n",
    "    return detokenizer.detokenize(desc)\n",
    "\n",
    "data[\"cleaned_description\"] = data[\"description\"].apply(clean_description)\n",
    "\n",
    "word_occurrence = data[\"cleaned_description\"].str.split(expand=True).stack().value_counts()\n",
    "\n",
    "total_words = sum(word_occurrence)\n",
    "\n",
    "# plot most common words \n",
    "\n",
    "top_words = word_occurrence[:30]/total_words\n",
    "\n",
    "ax = sns.barplot(x = top_words.values, y = top_words.index)\n",
    "\n",
    "# Setting title \n",
    "ax.set_title(\"% Occurrence of Most Frequent Words\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prices of Reviewed Wine by Country, Variety\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_by_country = data[[\"price\", \"country\"]].dropna(how = \"any\")\n",
    "\n",
    "# Remove all data greater than the 98th percentile\n",
    "val = prices_by_country[\"price\"].quantile(0.98)\n",
    "prices_by_country = prices_by_country[prices_by_country[\"price\"] < val]\n",
    "\n",
    "# Only consider countries where at least 100 wines have been reviewed\n",
    "prices_by_country = prices_by_country.groupby(\"country\").filter(lambda x: (x[\"price\"].count() >= 100))\n",
    "\n",
    "# Creating a boxplot\n",
    "ax = sns.boxplot(x=\"country\", y = \"price\", data=prices_by_country)\n",
    "\n",
    "# Setting title \n",
    "ax.set_title(\"Wine Prices by Country\")\n",
    "\n",
    "# Assuming prices are in USD since its an American website\n",
    "ax.set(xlabel = \"Origin Country\", ylabel = \"Price in USD\")\n",
    "\n",
    "# Making sure ticks aren't overlapping\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45,ha=\"right\",rotation_mode='anchor')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_by_variety = data[[\"price\", \"variety\"]].dropna(how = \"any\")\n",
    "\n",
    "# Remove all data greater than the 98th percentile\n",
    "val = prices_by_variety[\"price\"].quantile(0.98)\n",
    "prices_by_variety = prices_by_variety[prices_by_variety[\"price\"] < val]\n",
    "\n",
    "# Only consider varieties where at least 500 wines have been reviewed\n",
    "prices_by_variety = prices_by_variety.groupby(\"variety\").filter(lambda x: (x[\"price\"].count() >= 500))\n",
    "\n",
    "# Creating a boxplot\n",
    "ax = sns.boxplot(x=\"variety\", y = \"price\", data=prices_by_variety)\n",
    "\n",
    "# Setting title \n",
    "ax.set_title(\"Wine Prices by Variety\")\n",
    "\n",
    "# Assuming prices are in USD since its an American website\n",
    "ax.set(xlabel = \"Origin Country\", ylabel = \"Price in USD\")\n",
    "\n",
    "# Making sure ticks aren't overlapping\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=45,ha=\"right\",rotation_mode='anchor')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings versus Price \n",
    "\n",
    "Is there a correlation? \n",
    "\n",
    "(this takes a while to plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_by_price = data[[\"price\", \"points\"]]\n",
    "\n",
    "# Remove all data greater than the 98th percentile\n",
    "val = prices_by_variety[\"price\"].quantile(0.98)\n",
    "ratings_by_price = ratings_by_price[ratings_by_price[\"price\"] < val]\n",
    "\n",
    "ax = sns.lmplot(x = \"price\", y = \"points\", data = ratings_by_price)\n",
    "\n",
    "# Assuming prices are in USD since its an American website\n",
    "ax.set(xlabel = \"Price in USD\", ylabel = \"Wine Rating\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Wine Scores from Text Descriptions\n",
    "\n",
    "We use the following classification system provided by the website: \n",
    "\n",
    "**Classic:** 98-100\n",
    "\n",
    "**Superb:** 94-97\n",
    "\n",
    "**Excellent:** 90-93\n",
    "\n",
    "**Very Good:** 87-89\n",
    "\n",
    "**Good:** 83-86\n",
    "\n",
    "**Acceptable:** 80-82\n",
    "\n",
    "Since the website does not have a comprehensive list of scores, our classifier will only be able to differentiate between \"high quality\" wines. \n",
    "\n",
    "We will be using a RNN with GRU units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, Activation, CuDNNGRU, Bidirectional, Dropout, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Points to Classes \n",
    "\n",
    "def points_to_class(points):\n",
    "    if points in range(80,83):\n",
    "        return 0\n",
    "    elif points in range(83,87):\n",
    "        return 1\n",
    "    elif points in range(87,90):\n",
    "        return 2\n",
    "    elif points in range(90,94):\n",
    "        return 3\n",
    "    elif points in range(94-98):\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "data[\"rating\"] = data[\"points\"].apply(points_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Ratings are unbalanced \"\"\"\n",
    "\n",
    "# Class 4 is also missing\n",
    "data[\"rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New classification\n",
    "\n",
    "def points_to_class(points):\n",
    "    if points in range(80,83):\n",
    "        return 0\n",
    "    elif points in range(83,87):\n",
    "        return 1\n",
    "    elif points in range(87,90):\n",
    "        return 2\n",
    "    elif points in range(90,94):\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "    \n",
    "data[\"rating\"] = data[\"points\"].apply(points_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fixed parameters\n",
    "\n",
    "num_classes = 5\n",
    "embedding_dim = 300 \n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "max_len = 100\n",
    "\n",
    "class_weights = {0: 7,\n",
    "                1: 1,\n",
    "                2: 1, \n",
    "                3: 1,\n",
    "                4: 7}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encoding target\n",
    "def onehot(arr, num_class):\n",
    "    return np.eye(num_class)[np.array(arr.astype(int)).reshape(-1)]\n",
    "\n",
    "y = onehot(data[\"rating\"], num_classes)\n",
    "\n",
    "# Train, validation split (test is on another set)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(data[\"description\"], y, test_size = 0.05)\n",
    "\n",
    "\n",
    "# Prepare embeddings \n",
    "# Embedding file not contained in repo. Download from https://nlp.stanford.edu/projects/glove/\n",
    "# I am using glove.840B.300d.zip\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open(\"glove.840B.300d.txt\", encoding = \"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-embedding_dim])\n",
    "    coefs = np.asarray(values[-embedding_dim:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "    \n",
    "# train tokenizer \n",
    "tokenizer = Tokenizer(num_words = None)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# fit tokenizer\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(sequences_train, maxlen=max_len)\n",
    "\n",
    "sequences_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_val = pad_sequences(sequences_val, maxlen = max_len)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "                \n",
    "# create embedding layer \n",
    "# We can stipulate Out of Vocabulary word vectors here \n",
    "# In this case, they are initialized to zero vector\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "        \n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1, embedding_dim, weights = [embedding_matrix], input_length = max_len, trainable = False) \n",
    "input= Input(shape=(max_len, ), dtype = 'int32')\n",
    "embedded_sequences = embedding_layer(input) \n",
    "x = Bidirectional(CuDNNGRU(50, return_sequences=True))(embedded_sequences)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(50, activation = 'relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=input, outputs=output)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "checkpoint = ModelCheckpoint(\"model.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor='val_loss', mode='min', patience=3)\n",
    "callback = [checkpoint, early]\n",
    "        \n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), callbacks=callback, class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Testing Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test = pd.read_csv(\"winemag-data-130k-v2.csv\", index_col = False)\n",
    "test[\"rating\"] = test[\"points\"].apply(points_to_class)\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(test[\"description\"])\n",
    "X_test = pad_sequences(sequences_test, maxlen=max_len)\n",
    "\n",
    "# Predictions\n",
    "pred_test = model.predict(X_test)\n",
    "pred_test = [np.argmax(x) for x in pred_test]\n",
    "\n",
    "# Actual\n",
    "true_test = onehot(test[\"rating\"], num_class)\n",
    "true_test = [np.argmax(x) for x in true_test]\n",
    "\n",
    "# Find accuracies\n",
    "accuracy = accuracy_score(true_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total accuracy is \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(true_test, pred_test)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "class_name = [\"Acceptable\", \"Good\", \"Very Good\", \"Excellent\", \"Superb/Classic\"]\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_name))\n",
    "plt.xticks(tick_marks, class_name, rotation=45)\n",
    "plt.yticks(tick_marks, class_name)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
